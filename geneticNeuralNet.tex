\documentclass[twocolumn]{article}
\usepackage{lipsum,scrextend,mathtools,float,xspace,algorithm,algpseudocode,tabularx}
\usepackage[table]{xcolor}
\usepackage[backend=biber,sorting=none,style=ieee]{biblatex}
\addbibresource{geneticNeuralNet.bib}  

\newcommand\code[1]{
	\begin{minipage}{\textwidth}
		\texttt{\begin{addmargin}[0ex]{0ex}\scriptsize#1\end{addmargin}}
	\end{minipage}
}

\newcommand\pcode[5]{
	\begin{algorithm}[H] \caption{#1} \label{pcode:#2} \begin{algorithmic}[1]
		\Function{#3}{#4} 
			#5
		\EndFunction
	\end{algorithmic} \end{algorithm}
}

%figure(file name, label, scale, caption)
\newcommand\fig[5]{
	\begin{figure}[H]
		\begin{center}\includegraphics[scale=#3]{#1}\end{center}
		\caption{#4}\label{fig:#2}
	\end{figure}
}

\newcommand\figRef[1]{Figure \ref{fig:#1}\xspace}

\title{jGeneticNeuralNet: Multithreaded Training of Neural Networks by a Genetic Algorithm in Java}
\author{Eric Wimberley}
\begin{document}
\maketitle

\begin{abstract}
\lipsum[1]
\end{abstract}

\section{Introduction}
Genetic neural networks (GNNs) are learning algorithms that use neural networks to associate an input with an output. Miller et al were some of the first researchers to train neural networks with genetic algorithms~\cite{MillerToddHedge}. Rather than backpropagation, which determines the weights and biases within the network using partial derivatives, GNNs mutate the weights and biases randomly in a population of networks over many generations. 

\fig{images/visualization.png}{networkVis}{0.18}{
A visualization of a classification network with 8 input variables (green) and 10 output classes (blue). There are 5 hidden layers of 10 nodes each (black);
}

Genetic algorithms (GAs) are algorithms that use random mutations of solutions to optimimize some function of a system. They mimic genetic evolution of organisms in nature. While a random search of the problem space may seem less than optimal, it is can be easily parallelized~\cite{Tanese:1989:DGA:915973}.

More recent implementations allow for the structure of the network to mutate~\cite{LamStructure}. This allows networks to form more optimal structures for a particular problem space, as well as to optimize networks for size and complexity. For example, a GNN could automatically form a Convolutional Neural Network (CNN), which is optimal for problems such as handwritten character recognition~\cite{ConvolutionalCharacterClassification}, and classification on other complex data. 

\section{Network Model}
For extensibility, neurons are implemented as objects. Each neuron has the following fields:

\begin{itemize}
	\item A unique ID 
	\item A bias
	\item The activation function to use
	\item A map from IDs to input neurons
	\item A map from IDs to output neurons
 	\item A map from IDs to weights (IDs correspond to output map above)
\end{itemize}

Input neurons also include a field to contain the input feature. The activation function is overridden to return this input feature without any calculation. A neuron calculates its output by summing the outputs of all input neurons multiplied by their respective weights, adding the bias, and passing the result to an activation function~\cite{Russell:2003:AIM:773294}. 

$$\varphi((\sum_{n=1}^{N}n \times w_n)+bias)$$

A number of different activation functions are implemented. Functions with a range between 0.0 and 1.0 are particularly useful for probability regressions (the predicted probability of a class). Output neurons for the classification problem use these activation functions. 

%y=(sin(x*pi-pi/2)+1)/2
$$\varphi_{sin}(x) = (sin(x*\pi-\pi/2)+1)/2$$

\fig{images/sin.png}{sinact}{0.5}{
A plot of the $\varphi_{sin}(x)$ activation function.
}

%y=arctan(x)/Pi+0.5
$$\varphi_{arctan}(x) = arctan(x)/\pi+0.5$$

\fig{images/tan.png}{tanact}{0.5}{
A plot of the $\varphi_{arctan}(x)$ activation function.
}

\[
\varphi_{step}(x) =  
\begin{dcases}
    1,& \text{if } x\geq 0\\
    0,              & \text{otherwise}
\end{dcases}
\]

For regressions, some functions with a larger range are implemented.

$$\varphi_{linear}(x) = x$$

FIXME add additional activation functions x*x, etc...

\section{Genetic Algorithm}
Networks are trained with a genetic algorith, which first produces a large number of random networks. Each network is tested for fitness by determining its average error rate on the training data, and the population of networks is mutated to generate networks with improved fitness.

\subsection{Fitness}
\lipsum[1]

\subsection{Mutation}

\fig{images/originalRegression.png}{originalStructure}{0.2}{
A simple regression network that is fully connected.
}

\fig{images/mutatedRegression.png}{originalStructure}{0.2}{
A mutated network with a different connection structure.
}

\section{Optimizations}
The network training algorithm employs several important optimizations:

\begin{itemize}
 	\item Each network is trained in a seperate thread
	\item Memoization of neuron output  
	\item Random sub-sampling of the training data during training error estimation 
\end{itemize}

\subsection{Multithreaded Training}
\pcode{Training Algorithm}{train}{train}{}{
	\State Let pool be a thread pool that accepts jobs
	\State Let population be a set of networks
	\For{$nework$ in $population$} 
		\If {$condition$}
		\EndIf
	\EndFor
}

\subsection{Activation Function Memoization}

Highly connected networks use the output from the same neuron for multiple neurons in the next layer. Rather than recalculating the output of these neurons, the activation function output is memoized. Each neuron only needs to store two variables in order to accomplish this. Note that this optimization is important for both training and production use of the network.

\begin{itemize}
        \item The previous activation output 
        \item A flag indicating whether to use the memoized value
\end{itemize}

\fig{images/memoization.png}{memoization}{0.4}{
Two neurons use the output of neuron b. Instead of recomputing b each time, the output of b can be memoized. Note that $x_i$ refers to the bias for that neuron.
}

The structure from \figRef{memoization} occurs a large number of times in layered neural networks. While this optimization cannot compete with matrix multiplication with specialized hardware, it does mitigate some of the performance issues associated with object oriented neural networks. The instructions to compute the output of this network with and without memoization are shown below. \\

\noindent Without memoization: \\
\indent Step 1: Compute $\varphi(a+x1)$ \\
\indent Step 2: Compute $\varphi(b+x2)$ \\
\indent Step 3: Compute $\varphi(a+x1)$ \\
\indent Step 4: Compute $\varphi(b+x3)$ \\

\noindent With memoization: \\
\indent Step 1: Compute $\varphi(a+x1)$ \\
\indent Step 2: Compute $\varphi(b+x2)$ \\
\indent Step 3: Compute $\varphi(b+x3)$

For a fully connected neural network with $N$ layers and $M$ neurons per layer, memoization reduces predicion complexity from $O(M \times M \times N)$ to $O(M \times N)$. In other words, instead of computing the neuronal output for each conection for each layer, neuronal output is computed once per neuron. Not all networks are fully connected, but this is the worst case scenario for a non-memoized implementation. 

\section{Methods}
\lipsum[1]

\subsection{Usage}

TODO update with config object
\code{
String dataFile = "src/test/resources/iris.data"; \\
DataLoader dl = new DataLoader(); \\
dl.loadCSVFile(dataFile); \\
Learner<String> model = ClassificationGenticNeuralNetwork.train( \\
dl.getData(), dl.getClassLabels(), 1000, 500, 5, 8, 100.0);
}

\subsection{Tests}

\section{Results}

\begin{center}
\begin{tabular}{ l c c c }
Expected & Iris-versicolor & Iris-virginica & Iris-setosa \\
Iris-versicolor & 8 & 1 & 0 \\ 
Iris-virginica & 0 & 10 & 0 \\
Iris-setosa & 0 & 0 & 12
\end{tabular}
\end{center}

Accuracy: 0.967741935483871

\subsection{Classification}

\subsection{Regression}

\section{Conclusion}
\lipsum[3]

\printbibliography

\end{document}
